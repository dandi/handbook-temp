{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to the DANDI archive documentation The Web interface to the DANDI archive is located at https://dandiarchive.org . This site provides information on how to interact with the archive. Where to communicate: For general discussion post on https://neurostars.org and use the tag dandi . Report DANDI related bugs here . Register on DANDI using Github and we will invite you to the DANDI Slack workspace. See here for details on how to register . Email us: info@dandiarchive.org How to use this documentation To start using the archive head over to: Working with DANDI If you want to know more about the DANDI project, its goals and the problems it tries to solve: check out our introduction . The DANDI archive stores cellular neurophysiology datasets using NWB, BIDS, and other BRAIN Initiative standards. If you are unfamiliar with such things, head over to our FAQ . You do not need an in depth understanding of what those things are to use the DANDI archive but some \"big picture\" conceptual understanding could save you from a lot of confusion. \ud83d\ude09 Not sure how the project is organized? Check out the project structure page. Licence This work is licensed under a Creative Commons Attribution 4.0 International License . Contributing and feedback We are looking for people to give us feedback on this documentation if anything is unclear by opening an issue on our repository . You can also get in touch on our Slack channel. You will be invited once you [register an account] on the archive. If you want to get started right away and contribute directly to this documentation, you can find references and how-to in the about section .","title":"Welcome"},{"location":"#welcome-to-the-dandi-archive-documentation","text":"The Web interface to the DANDI archive is located at https://dandiarchive.org . This site provides information on how to interact with the archive.","title":"Welcome to the DANDI archive documentation"},{"location":"#where-to-communicate","text":"For general discussion post on https://neurostars.org and use the tag dandi . Report DANDI related bugs here . Register on DANDI using Github and we will invite you to the DANDI Slack workspace. See here for details on how to register . Email us: info@dandiarchive.org","title":"Where to communicate:"},{"location":"#how-to-use-this-documentation","text":"To start using the archive head over to: Working with DANDI If you want to know more about the DANDI project, its goals and the problems it tries to solve: check out our introduction . The DANDI archive stores cellular neurophysiology datasets using NWB, BIDS, and other BRAIN Initiative standards. If you are unfamiliar with such things, head over to our FAQ . You do not need an in depth understanding of what those things are to use the DANDI archive but some \"big picture\" conceptual understanding could save you from a lot of confusion. \ud83d\ude09 Not sure how the project is organized? Check out the project structure page.","title":"How to use this documentation"},{"location":"#licence","text":"This work is licensed under a Creative Commons Attribution 4.0 International License .","title":"Licence"},{"location":"#contributing-and-feedback","text":"We are looking for people to give us feedback on this documentation if anything is unclear by opening an issue on our repository . You can also get in touch on our Slack channel. You will be invited once you [register an account] on the archive. If you want to get started right away and contribute directly to this documentation, you can find references and how-to in the about section .","title":"Contributing and feedback"},{"location":"01_introduction/","text":"Introduction Advantages of using DANDI An open data archive to submit cellular neurophysiology data. A persistent, versioned and growing collection of standardized cellular neurophysiology data. Rich metadata to support search across data. A place to house data to collaborate across research sites. Consistent and transparent data standards to simplify software development. Supported by the BRAIN Initiative and the AWS Public dataset programs. The challenges To know which data are useful, data has to be accessible. Non standardized datasets lead to significant resources needed to understand and adapt code to these datasets. The multitude of different hardware platforms and custom binary formats requires significant effort to consolidate into reusable datasets. There are many domain general places to house data (e.g., Open Science Framework, G-Node, Dropbox, Google drive), but it is difficult to find relevant datasets. Datasets are growing larger requiring compute services to be closer to data. Neurotechnology is evolving and requires flexible extensions to metadata and data storage requirements. Consolidating and creating robust algorithms (e.g., spike sorting) requires varied data sources. Our solution We have developed a FAIR (Findable, Accessible, Interoperable, Reusable) data archive to house standardized cellular neurophysiology and associated data. We use the Neurodata Without Borders , Brain Imaging Data Structure , Neuroimaging Data Model and other BRAIN Initiative standards to organize and search the data. A Jupyterhub-based analysis platform provides easy access to the data. The data can be accessed programmatically allowing for new software and tools to be built. The archive itself is built on a software stack of open source products, thus enriching the ecosystem. The archive provides persistent identifiers for versioned datasets thus improving reproducibility of neurophysiology research.","title":"Introduction"},{"location":"01_introduction/#introduction","text":"","title":"Introduction"},{"location":"01_introduction/#advantages-of-using-dandi","text":"An open data archive to submit cellular neurophysiology data. A persistent, versioned and growing collection of standardized cellular neurophysiology data. Rich metadata to support search across data. A place to house data to collaborate across research sites. Consistent and transparent data standards to simplify software development. Supported by the BRAIN Initiative and the AWS Public dataset programs.","title":"Advantages of using DANDI"},{"location":"01_introduction/#the-challenges","text":"To know which data are useful, data has to be accessible. Non standardized datasets lead to significant resources needed to understand and adapt code to these datasets. The multitude of different hardware platforms and custom binary formats requires significant effort to consolidate into reusable datasets. There are many domain general places to house data (e.g., Open Science Framework, G-Node, Dropbox, Google drive), but it is difficult to find relevant datasets. Datasets are growing larger requiring compute services to be closer to data. Neurotechnology is evolving and requires flexible extensions to metadata and data storage requirements. Consolidating and creating robust algorithms (e.g., spike sorting) requires varied data sources.","title":"The challenges"},{"location":"01_introduction/#our-solution","text":"We have developed a FAIR (Findable, Accessible, Interoperable, Reusable) data archive to house standardized cellular neurophysiology and associated data. We use the Neurodata Without Borders , Brain Imaging Data Structure , Neuroimaging Data Model and other BRAIN Initiative standards to organize and search the data. A Jupyterhub-based analysis platform provides easy access to the data. The data can be accessed programmatically allowing for new software and tools to be built. The archive itself is built on a software stack of open source products, thus enriching the ecosystem. The archive provides persistent identifiers for versioned datasets thus improving reproducibility of neurophysiology research.","title":"Our solution"},{"location":"100_about_this_doc/","text":"About this documentation This documentation is a work in progress and we welcome any input: if something is missing or unclear, let us know by opening an issue on our repository . Serving the docs locally This project uses the MkDocs tool with the Material theme and extra plugins to generate the website. To test locally, you will need to install the Python dependencies. To do that, type the following commands: git clone https://github.com/dandi/handbook.git cd handbook pip install -r requirements.txt If you are working on your fork , simply replace https://github.com/dandi/handbook.git by git clone git@github.com/<username>/handbook.git where <username> is your GitHub username Once done, you need to run MkDocs. Simply type: mkdocs serve Finally, open up http://127.0.0.1:8000/ in your browser, and you should see the default home page of the documentation being displayed.","title":"About this doc"},{"location":"100_about_this_doc/#about-this-documentation","text":"This documentation is a work in progress and we welcome any input: if something is missing or unclear, let us know by opening an issue on our repository .","title":"About this documentation"},{"location":"100_about_this_doc/#serving-the-docs-locally","text":"This project uses the MkDocs tool with the Material theme and extra plugins to generate the website. To test locally, you will need to install the Python dependencies. To do that, type the following commands: git clone https://github.com/dandi/handbook.git cd handbook pip install -r requirements.txt If you are working on your fork , simply replace https://github.com/dandi/handbook.git by git clone git@github.com/<username>/handbook.git where <username> is your GitHub username Once done, you need to run MkDocs. Simply type: mkdocs serve Finally, open up http://127.0.0.1:8000/ in your browser, and you should see the default home page of the documentation being displayed.","title":"Serving the docs locally"},{"location":"10_using_dandi/","text":"Working with DANDI DANDI provides access to an archive that stores cellular neurophysiology datasets. We refer to such datasets as Dandisets . A Dandiset is organized in a structured manner to help users and software tools interact with it. Each Dandiset has a unique persistent identifier that you can use to go directly to the Dandiset (e.g., https://identifiers.org/DANDI:000004 ). You can use this identifier to cite the Dandiset in your publications or provide direct access to a Dandiset . DANDI components The DANDI Web application The DANDI Web application allows you to: Browse Dandisets . Search across Dandisets . Create an account to register a new Dandiset or gain access to the Dandihub analysis platform . Add collaborators to your Dandiset . Retrieve an API key to perform data upload to your Dandisets . Publish versions of your Dandisets . The DANDI Python client The DANDI Python client allows you to: Download Danidsets and individual subject folders or files. Organize your data locally before upload. Upload Dandisets . The Dandihub analysis platform Dandihub provides a Jupyter environment to interact with the DANDI archive. To use the hub, you will need to register an account using the DANDI Web application . Please note that Dandihub is not intended for significant computation, but provides a place to introspect Dandisets and files. Downloading from DANDI You can download entire Dandisets or single files. Downloading a file Using the Web application Each Dandiset has a View Data option. This provides a folder-like view to navigate a Dandiset . Any file in the Dandiset has a download icon next to it. You can click this icon to download a file to your device where you are browsing or right click to get the download URL of the file. You can then use this URL programmatically or in other applications such as the NWB Explorer or in a Jupyter notebook on Dandihub . Using the Python CLI First install the Python client using pip install dandi in a Python 3.7+ environment. Downloading a Dandiset , e.g.: dandi download https://identifiers.org/DANDI:000004 Downloading data for a specific subject from a dandiset (names of the subjects could be found on the gui.dandiarchive.org website or by running dandi ls -r https://identifiers.org/DANDI:000004 ), e.g.: dandi download https://api.dandiarchive.org/api/dandisets/000004/versions/draft/assets/?path=sub-P10HMH Downloading a specific file from a dandiset (a link for the specific file could be found on the gui.dandiarchive.org website), e.g.: dandi download https://api.dandiarchive.org/api/dandisets/000004/versions/draft/assets/9d9f379d-9fd1-4872-8c49-3891dfb693ab/download/ Create an account on DANDI To create an account on DANDI, you will need to. Create a Github account if you don't have one. Using your Github account register a DANDI account . You will receive an email acknowledging activation of your account within 24 hours. You can now login to DANDI using the Github by clicking on the login button. Uploading a Dandiset Setup If you do not have a DANDI account, please create an account Log in to DANDI and copy your API key. This is under your user initials on the top right after logging in. Locally Create a Python environment (e.g., Miniconda, virtualenv) Install the DANDI CLI into your Python environment pip install \"dandi>=0.14\" Make sure you have version 0.14.2 or higher Store your API key somewhere that the CLI can find it; see \"Storing Access Credentials\" below. Data upload/management workflow Register a dandiset to generate an identifier. You will be asked to enter basic metadata, a name (title) and description (abstract) for your dataset. Click New Dataset in the Web application after logging in. After you are done, note the dataset identifer. We will call this <dataset_id> . Convert your data to NWB 2.1+ in a local folder. Let's call this <source_folder> This step can be complex depending on your data. Feel free to reach out to us for help . Validate the NWB files by running: dandi validate <source_folder> Preparing a dataset folder for upload: dandi download https://dandiarchive.org/dandiset/<dataset_id>/draft cd <dataset_id> dandi organize <source_folder> -f dry dandi organize <source_folder> dandi upload Add metadata on the Web. Click on the Edit metadata link by visiting your dandiset landing page: https://dandiarchive.org/dandiset/<dataset_id>/draft Use the dandiset URL in your preprint directly, or download it using the dandi CLI: dandi download https://dandiarchive.org/dandiset/<dataset_id>/draft Storing Access Credentials By default, the DANDI CLI looks for an API key in the DANDI_API_KEY environment variable. If this is not set, the CLI will look up the API key using the keyring library, which supports numerous backends, including the system keyring, an encrypted keyfile, and a plaintext (unencrypted) keyfile. You can store your API key where the keyring library can find it by using the keyring program: Run keyring set dandi-api-dandi key and enter the API key when asked for the password for key in dandi-api-dandi . You can set the backend the keyring library uses either by setting the PYTHON_KEYRING_BACKEND environment variable or by filling in the keyring library's configuration file . IDs for the available backends can be listed by running keyring --list . If no backend is specified in this way, the library will use the available backend with the highest priority. If the API key isn't stored in either the DANDI_API_KEY environment variable or in the keyring, the CLI will prompt you to enter the API key, and then it will store it in the keyring. This may cause you to be prompted further; you may be asked to enter a password to encrypt/decrypt the keyring, or you may be asked by your OS to confirm whether to give the DANDI CLI access to the keyring. If the DANDI CLI encounters an error while attempting to fetch the API key from the default keyring backend, it will fall back to using an encrypted keyfile (the keyrings.alt.file.EncryptedKeyring backend). If the keyfile does not already exist, the CLI will ask you for confirmation; if you answer \"yes,\" the keyring configuration file (if does not already exist; see above) will be configured to use EncryptedKeyring as the default backend. If you answer \"no,\" the CLI will exit with an error, and you must store the API key somewhere accessible to the CLI on your own. Publish a Dandiset \ud83d\udee0 Work in progress \ud83d\udee0","title":"Using DANDI"},{"location":"10_using_dandi/#working-with-dandi","text":"DANDI provides access to an archive that stores cellular neurophysiology datasets. We refer to such datasets as Dandisets . A Dandiset is organized in a structured manner to help users and software tools interact with it. Each Dandiset has a unique persistent identifier that you can use to go directly to the Dandiset (e.g., https://identifiers.org/DANDI:000004 ). You can use this identifier to cite the Dandiset in your publications or provide direct access to a Dandiset .","title":"Working with DANDI"},{"location":"10_using_dandi/#dandi-components","text":"","title":"DANDI components"},{"location":"10_using_dandi/#the-dandi-web-application","text":"The DANDI Web application allows you to: Browse Dandisets . Search across Dandisets . Create an account to register a new Dandiset or gain access to the Dandihub analysis platform . Add collaborators to your Dandiset . Retrieve an API key to perform data upload to your Dandisets . Publish versions of your Dandisets .","title":"The DANDI Web application"},{"location":"10_using_dandi/#the-dandi-python-client","text":"The DANDI Python client allows you to: Download Danidsets and individual subject folders or files. Organize your data locally before upload. Upload Dandisets .","title":"The DANDI Python client"},{"location":"10_using_dandi/#the-dandihub-analysis-platform","text":"Dandihub provides a Jupyter environment to interact with the DANDI archive. To use the hub, you will need to register an account using the DANDI Web application . Please note that Dandihub is not intended for significant computation, but provides a place to introspect Dandisets and files.","title":"The Dandihub analysis platform"},{"location":"10_using_dandi/#downloading-from-dandi","text":"You can download entire Dandisets or single files.","title":"Downloading from DANDI"},{"location":"10_using_dandi/#downloading-a-file","text":"","title":"Downloading a file"},{"location":"10_using_dandi/#using-the-web-application","text":"Each Dandiset has a View Data option. This provides a folder-like view to navigate a Dandiset . Any file in the Dandiset has a download icon next to it. You can click this icon to download a file to your device where you are browsing or right click to get the download URL of the file. You can then use this URL programmatically or in other applications such as the NWB Explorer or in a Jupyter notebook on Dandihub .","title":"Using the Web application"},{"location":"10_using_dandi/#using-the-python-cli","text":"First install the Python client using pip install dandi in a Python 3.7+ environment. Downloading a Dandiset , e.g.: dandi download https://identifiers.org/DANDI:000004 Downloading data for a specific subject from a dandiset (names of the subjects could be found on the gui.dandiarchive.org website or by running dandi ls -r https://identifiers.org/DANDI:000004 ), e.g.: dandi download https://api.dandiarchive.org/api/dandisets/000004/versions/draft/assets/?path=sub-P10HMH Downloading a specific file from a dandiset (a link for the specific file could be found on the gui.dandiarchive.org website), e.g.: dandi download https://api.dandiarchive.org/api/dandisets/000004/versions/draft/assets/9d9f379d-9fd1-4872-8c49-3891dfb693ab/download/","title":"Using the Python CLI"},{"location":"10_using_dandi/#create-an-account-on-dandi","text":"To create an account on DANDI, you will need to. Create a Github account if you don't have one. Using your Github account register a DANDI account . You will receive an email acknowledging activation of your account within 24 hours. You can now login to DANDI using the Github by clicking on the login button.","title":"Create an account on DANDI"},{"location":"10_using_dandi/#uploading-a-dandiset","text":"Setup If you do not have a DANDI account, please create an account Log in to DANDI and copy your API key. This is under your user initials on the top right after logging in. Locally Create a Python environment (e.g., Miniconda, virtualenv) Install the DANDI CLI into your Python environment pip install \"dandi>=0.14\" Make sure you have version 0.14.2 or higher Store your API key somewhere that the CLI can find it; see \"Storing Access Credentials\" below. Data upload/management workflow Register a dandiset to generate an identifier. You will be asked to enter basic metadata, a name (title) and description (abstract) for your dataset. Click New Dataset in the Web application after logging in. After you are done, note the dataset identifer. We will call this <dataset_id> . Convert your data to NWB 2.1+ in a local folder. Let's call this <source_folder> This step can be complex depending on your data. Feel free to reach out to us for help . Validate the NWB files by running: dandi validate <source_folder> Preparing a dataset folder for upload: dandi download https://dandiarchive.org/dandiset/<dataset_id>/draft cd <dataset_id> dandi organize <source_folder> -f dry dandi organize <source_folder> dandi upload Add metadata on the Web. Click on the Edit metadata link by visiting your dandiset landing page: https://dandiarchive.org/dandiset/<dataset_id>/draft Use the dandiset URL in your preprint directly, or download it using the dandi CLI: dandi download https://dandiarchive.org/dandiset/<dataset_id>/draft","title":"Uploading a Dandiset"},{"location":"10_using_dandi/#storing-access-credentials","text":"By default, the DANDI CLI looks for an API key in the DANDI_API_KEY environment variable. If this is not set, the CLI will look up the API key using the keyring library, which supports numerous backends, including the system keyring, an encrypted keyfile, and a plaintext (unencrypted) keyfile. You can store your API key where the keyring library can find it by using the keyring program: Run keyring set dandi-api-dandi key and enter the API key when asked for the password for key in dandi-api-dandi . You can set the backend the keyring library uses either by setting the PYTHON_KEYRING_BACKEND environment variable or by filling in the keyring library's configuration file . IDs for the available backends can be listed by running keyring --list . If no backend is specified in this way, the library will use the available backend with the highest priority. If the API key isn't stored in either the DANDI_API_KEY environment variable or in the keyring, the CLI will prompt you to enter the API key, and then it will store it in the keyring. This may cause you to be prompted further; you may be asked to enter a password to encrypt/decrypt the keyring, or you may be asked by your OS to confirm whether to give the DANDI CLI access to the keyring. If the DANDI CLI encounters an error while attempting to fetch the API key from the default keyring backend, it will fall back to using an encrypted keyfile (the keyrings.alt.file.EncryptedKeyring backend). If the keyfile does not already exist, the CLI will ask you for confirmation; if you answer \"yes,\" the keyring configuration file (if does not already exist; see above) will be configured to use EncryptedKeyring as the default backend. If you answer \"no,\" the CLI will exit with an error, and you must store the API key somewhere accessible to the CLI on your own.","title":"Storing Access Credentials"},{"location":"10_using_dandi/#publish-a-dandiset","text":"\ud83d\udee0 Work in progress \ud83d\udee0","title":"Publish a Dandiset"},{"location":"20_project_structure/","text":"Project structure The DANDI project is organized around several Github repositories. The main ones are the following. The DANDI archive. This repository contains the code for deploying the archive. It includes the client-side Web application frontend based on the Vuejs framework, the server backend extensions to the Girder platform , and the deployment code for pushing changes to the archive as they are merged in. The DANDI Python client. This repository contains the code for the command line tool used to interact with the archive. It allows you to download data from the archive. It also allows you to locally organize and validate your data before uploading to the archive. The DANDI Jupyterhub. This repository contains the code for deploying a Jupyterhub instance to support interaction with the DANDI archive. The DANDI API. This repository provides the code for the DANDI API. The DANDI schema. This repostiory provides the details and some supporting code for the DANDI metadata schema. The DANDI handbook. This repository provides the contents of this website. The DANDI Website. This repository provides an overview of the DANDI project and the team members and collaborators.","title":"Project structure"},{"location":"20_project_structure/#project-structure","text":"The DANDI project is organized around several Github repositories. The main ones are the following. The DANDI archive. This repository contains the code for deploying the archive. It includes the client-side Web application frontend based on the Vuejs framework, the server backend extensions to the Girder platform , and the deployment code for pushing changes to the archive as they are merged in. The DANDI Python client. This repository contains the code for the command line tool used to interact with the archive. It allows you to download data from the archive. It also allows you to locally organize and validate your data before uploading to the archive. The DANDI Jupyterhub. This repository contains the code for deploying a Jupyterhub instance to support interaction with the DANDI archive. The DANDI API. This repository provides the code for the DANDI API. The DANDI schema. This repostiory provides the details and some supporting code for the DANDI metadata schema. The DANDI handbook. This repository provides the contents of this website. The DANDI Website. This repository provides an overview of the DANDI project and the team members and collaborators.","title":"Project structure"},{"location":"30_schema/","text":"The metadata schema The core model of DANDI is based on DATS, Datacite, schema.org, and the C2M2 effort. This page describes the properties of the current objects. Common metadata Dandiset specific extesnions Asset specific extensions","title":"The metadata schema"},{"location":"30_schema/#the-metadata-schema","text":"The core model of DANDI is based on DATS, Datacite, schema.org, and the C2M2 effort. This page describes the properties of the current objects.","title":"The metadata schema"},{"location":"30_schema/#common-metadata","text":"","title":"Common metadata"},{"location":"30_schema/#dandiset-specific-extesnions","text":"","title":"Dandiset specific extesnions"},{"location":"30_schema/#asset-specific-extensions","text":"","title":"Asset specific extensions"},{"location":"40_development/","text":"Developers information Current deployed instance Uses Girder as a backend for storing dandisets on S3 in s3://dandiarchive with Girder keystore under girder-assetstore/ https://github.com/dandi/dandiarchive - Web UI https://gui.dandiarchive.org/ is the deployed web UI instance https://github.com/dandi/dandi-cli/ - CLI/Python client To be installed via pip or conda to interact with the archive. https://github.com/dandi/redirector - redirector which provides redirects from https://dandiarchive.org/ In development It will use dandi-api service/backend which would provide specialized to dandiarchive functionality to upload/download dandisets and public releases. All data ATM goes into a private bucket but later will migrate to use s3://dandiarchive https://github.com/dandi/dandiarchive - Web UI The same repository used as for the deployed instance but in a different \"setup\" so it uses dandi-api https://gui-beta-dandiarchive-org.netlify.app/ is the deployed Web UI instance working against dandi-api https://github.com/dandi/dandi-api/ - DANDI API service/backend is DJANGO-based implementation of the DANDI API service to which both web UI (https://github.com/dandi/dandiarchive) https://api.dandiarchive.org/swagger/ provides swagger interface to see and play around with the API server https://github.com/dandi/dandi-cli/ - CLI/Python client The same client, but with DANDI_DEVEL=1 env variable would enable command line options to interact with dandi-api instead of a deployed instance. https://github.com/dandi/schema - exported DANDI schema Contains json schemas for DANDI metata schema defined and exported by/from https://github.com/dandi/dandi-cli/ where it resides in https://github.com/dandi/dandi-cli/blob/master/dandi/models.py and https://github.com/dandi/dandi-cli/blob/master/dandi/model_types.py http://github.com/dandi/dandi-api-datasets - CI dataset for metadata conversion & validation Automatically updated (on drogon) given the current state of dandisets in the archive and dandi-cli (schema etc)","title":"Developers information"},{"location":"40_development/#developers-information","text":"","title":"Developers information"},{"location":"40_development/#current-deployed-instance","text":"Uses Girder as a backend for storing dandisets on S3 in s3://dandiarchive with Girder keystore under girder-assetstore/","title":"Current deployed instance"},{"location":"40_development/#httpsgithubcomdandidandiarchive-web-ui","text":"https://gui.dandiarchive.org/ is the deployed web UI instance","title":"https://github.com/dandi/dandiarchive - Web UI"},{"location":"40_development/#httpsgithubcomdandidandi-cli-clipython-client","text":"To be installed via pip or conda to interact with the archive.","title":"https://github.com/dandi/dandi-cli/ - CLI/Python client"},{"location":"40_development/#httpsgithubcomdandiredirector-redirector","text":"which provides redirects from https://dandiarchive.org/","title":"https://github.com/dandi/redirector - redirector"},{"location":"40_development/#in-development","text":"It will use dandi-api service/backend which would provide specialized to dandiarchive functionality to upload/download dandisets and public releases. All data ATM goes into a private bucket but later will migrate to use s3://dandiarchive","title":"In development"},{"location":"40_development/#httpsgithubcomdandidandiarchive-web-ui_1","text":"The same repository used as for the deployed instance but in a different \"setup\" so it uses dandi-api https://gui-beta-dandiarchive-org.netlify.app/ is the deployed Web UI instance working against dandi-api","title":"https://github.com/dandi/dandiarchive - Web UI"},{"location":"40_development/#httpsgithubcomdandidandi-api-dandi-api-servicebackend","text":"is DJANGO-based implementation of the DANDI API service to which both web UI (https://github.com/dandi/dandiarchive) https://api.dandiarchive.org/swagger/ provides swagger interface to see and play around with the API server","title":"https://github.com/dandi/dandi-api/ - DANDI API service/backend"},{"location":"40_development/#httpsgithubcomdandidandi-cli-clipython-client_1","text":"The same client, but with DANDI_DEVEL=1 env variable would enable command line options to interact with dandi-api instead of a deployed instance.","title":"https://github.com/dandi/dandi-cli/ - CLI/Python client"},{"location":"40_development/#httpsgithubcomdandischema-exported-dandi-schema","text":"Contains json schemas for DANDI metata schema defined and exported by/from https://github.com/dandi/dandi-cli/ where it resides in https://github.com/dandi/dandi-cli/blob/master/dandi/models.py and https://github.com/dandi/dandi-cli/blob/master/dandi/model_types.py","title":"https://github.com/dandi/schema - exported DANDI schema"},{"location":"40_development/#httpgithubcomdandidandi-api-datasets-ci-dataset-for-metadata-conversion-validation","text":"Automatically updated (on drogon) given the current state of dandisets in the archive and dandi-cli (schema etc)","title":"http://github.com/dandi/dandi-api-datasets - CI dataset for metadata conversion &amp; validation"},{"location":"98_FAQ/","text":"FAQ Who is DANDI for? DANDI can be useful to any individuals interested in neuroscience and/or large and diverse datascience challenges.","title":"FAQ"},{"location":"98_FAQ/#faq","text":"","title":"FAQ"},{"location":"98_FAQ/#who-is-dandi-for","text":"DANDI can be useful to any individuals interested in neuroscience and/or large and diverse datascience challenges.","title":"Who is DANDI for?"},{"location":"99_glossary/","text":"Glossary Asset BIDS Dandiset NIDM NWB","title":"Glossary"},{"location":"99_glossary/#glossary","text":"Asset BIDS Dandiset NIDM NWB","title":"Glossary"}]}